{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "C0b1zJ0xXLq69JWddpGTCF",
     "type": "MD"
    }
   },
   "source": [
    "Установка необходимых системных библиотек (кодеков для работы с изображением)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "VnUM2iFULDwlGMan5c1iWS",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "#!sudo apt-get update && sudo apt-get install ffmpeg libsm6 libxext6 -y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "muETdQL9p5vDycVzRtFFoE",
     "type": "MD"
    }
   },
   "source": [
    "Установка необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "o6Yv1JiSXu7wsFrpntGFew",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "#%pip install jupyterlab ultralytics torch cvzone pandas openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "N4YZ1hRakOgzuJkVVVRGUh",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary (fused): 168 layers, 3006038 parameters, 0 gradients\n",
      "Model summary (fused): 168 layers, 3006038 parameters, 0 gradients\n",
      "Словари состояния усреднены и успешно сохранены.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Загрузка первой модели\n",
    "model1 = YOLO(r'best_1.pt')\n",
    "if model1.model:\n",
    "    model1 = model1.model.to('cpu').fuse()\n",
    "else:\n",
    "    print(\"Ошибка при загрузке первой модели\")\n",
    "\n",
    "# Загрузка второй модели\n",
    "model2 = YOLO(r'best_2.pt')\n",
    "if model2.model:\n",
    "    model2 = model2.model.to('cpu').fuse()\n",
    "else:\n",
    "    print(\"Ошибка при загрузке второй модели\")\n",
    "\n",
    "merged_model_states_file_name = 'merged_yolov8.pth'\n",
    "\n",
    "# Проверка успешности загрузки обеих моделей\n",
    "if model1 and model2:\n",
    "    # Усреднение словарей состояния\n",
    "    model1_state_dict = model1.state_dict()\n",
    "    model2_state_dict = model2.state_dict()\n",
    "\n",
    "    new_state_dict = {}\n",
    "    for k in model1_state_dict.keys():\n",
    "        new_state_dict[k] = (model1_state_dict[k] + model2_state_dict[k]) / 2\n",
    "\n",
    "    # Сохранение усредненного словаря состояния\n",
    "    torch.save({'model': new_state_dict}, merged_model_states_file_name)\n",
    "\n",
    "    print(\"Словари состояния усреднены и успешно сохранены.\")\n",
    "else:\n",
    "    print(\"Невозможно усреднить словари состояния из-за отсутствия моделей.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "Fa6Iwu5ARQytu6ORCVJoJg",
     "type": "MD"
    }
   },
   "source": [
    "Теперь у нас есть контрольная точка: файл с сохранёнными весами. Сымитируем передачу этой информации в другое окружение и обучение новой модели.\n",
    "\n",
    "Создаём третью модель из файла с сохранёнными усреднёнными состояниями.\n",
    "\n",
    "Этот код загружает усредненный словарь состояния модели из файла merged_model_states_file_name и сохраняет его в переменную merged_model_state_dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "yjGJD3Aj4CuO0beWrGlrY2",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "states = torch.load(merged_model_states_file_name, map_location=torch.device('cpu'))\n",
    "merged_model_state_dict = states['model']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "GbccCtUV893GBbRvhoUi63",
     "type": "MD"
    }
   },
   "source": [
    "Ключевой момент - состояния, сохранённые с помощью torch, не являются сериализованной YOLO моделью, в отличие от best_1.pt и best_2.pt. \n",
    "\n",
    "Поэтому для получения модели из усреднённой модели создаём копию модели 2, после чего подгружаем веса, экспортированные на предыдущем шаге."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "OkdUE3Eaezp1oACRHmV6lF",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary (fused): 168 layers, 3006038 parameters, 0 gradients\n",
      "Модель успешно загружена.\n"
     ]
    }
   ],
   "source": [
    "model3 = YOLO(r'best_2.pt')\n",
    "model3.fuse()\n",
    "model3.model.load_state_dict(merged_model_state_dict)\n",
    "\n",
    "# Проверка успешной загрузки модели\n",
    "if model3.model:\n",
    "    print(\"Модель успешно загружена.\")\n",
    "else:\n",
    "    print(\"Ошибка при загрузке модели.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "6HhNIXsbiXWXYVG9DtCRMe",
     "type": "MD"
    }
   },
   "source": [
    "Теперь можно использовать model3 для распознавания. \n",
    "\n",
    "Ещё один нюанс - при использовании jupyter блокнота в datalore не будет работать вывод картинок на экран, поэтому добавим переменную have_display, которая будет True только в ситуации, когда запускаем код локально, без jupyter блокнота, а как python код."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "M7kIpbn6R0EAzNdDJ02V9y",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "have_display = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 crop, 1229.0ms\n",
      "Speed: 19.0ms preprocess, 1229.0ms inference, 2473.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muntadher\\AppData\\Local\\Temp\\ipykernel_7528\\1677540794.py:67: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame({'Class': classNames[cls], 'Confidence': conf, 'X1': x1, 'Y1': y1, 'X2': x2, 'Y2': y2}, index=[0])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 640x640 1 crop, 1140.0ms\n",
      "Speed: 17.0ms preprocess, 1140.0ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 crop, 613.0ms\n",
      "Speed: 12.0ms preprocess, 613.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 crop, 625.0ms\n",
      "Speed: 12.0ms preprocess, 625.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 crop, 799.0ms\n",
      "Speed: 21.0ms preprocess, 799.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 crop, 516.0ms\n",
      "Speed: 33.0ms preprocess, 516.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 crop, 488.0ms\n",
      "Speed: 32.0ms preprocess, 488.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 crop, 436.0ms\n",
      "Speed: 21.0ms preprocess, 436.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 crop, 475.0ms\n",
      "Speed: 10.0ms preprocess, 475.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 crop, 453.0ms\n",
      "Speed: 20.0ms preprocess, 453.0ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 crop, 443.0ms\n",
      "Speed: 11.0ms preprocess, 443.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import time\n",
    "import cvzone\n",
    "from copy import deepcopy\n",
    "from ultralytics import YOLO\n",
    "\n",
    "classNames = ['crop', 'weed']  # Список классов объектов\n",
    "\n",
    "# Загрузка исходного видео\n",
    "cap = cv2.VideoCapture('pred_t.mp4')\n",
    "\n",
    "# Загрузка модели из объединенного словаря состояния\n",
    "model3 = YOLO(r'best_2.pt')\n",
    "\n",
    "# Проверка успешного открытия видео\n",
    "if not cap.isOpened():\n",
    "    print(\"Ошибка при открытии видеофайла.\")\n",
    "    exit()\n",
    "\n",
    "# Получение параметров видео\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Определение кодека и создание объекта VideoWriter\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('output_video.avi', fourcc, fps, (width, height))\n",
    "\n",
    "# Создание DataFrame для хранения обнаруженных объектов\n",
    "df = pd.DataFrame(columns=['Class', 'Confidence', 'X1', 'Y1', 'X2', 'Y2'])\n",
    "\n",
    "# Инициализация переменных для расчета FPS\n",
    "prev_frame_time = 0\n",
    "new_frame_time = 0\n",
    "\n",
    "# Определение переменной have_display\n",
    "have_display = True\n",
    "\n",
    "while True:\n",
    "    new_frame_time = time.time()\n",
    "\n",
    "    # Захват кадра\n",
    "    success, img = cap.read()\n",
    "\n",
    "    if not success:\n",
    "        print(\"Ошибка при считывании кадра из видеопотока.\")\n",
    "        break\n",
    "\n",
    "    # Обнаружение объектов\n",
    "    results = model3(img)\n",
    "\n",
    "    for r in results:\n",
    "        for b in r.boxes:\n",
    "            x1, y1, x2, y2 = int(b.xyxy[0][0]), int(b.xyxy[0][1]), int(b.xyxy[0][2]), int(b.xyxy[0][3])\n",
    "            conf = float(b.conf)\n",
    "            cls = int(b.cls)\n",
    "\n",
    "            try:\n",
    "                if 0 <= cls < len(classNames):\n",
    "                    cvzone.cornerRect(img, (x1, y1, x2 - x1, y2 - y1))\n",
    "                    cvzone.putTextRect(img, f'{classNames[cls]} {conf:.2f}', (max(0, x1), max(35, y1)), scale=1, thickness=1)\n",
    "                else:\n",
    "                    print(f\"Предупреждение: Индекс класса {cls} вышел за пределы диапазона. Используется метка по умолчанию.\")\n",
    "                    cvzone.putTextRect(img, \"Неизвестный класс\", (max(0, x1), max(35, y1)), scale=1, thickness=1)\n",
    "\n",
    "                df = pd.concat([df, pd.DataFrame({'Class': classNames[cls], 'Confidence': conf, 'X1': x1, 'Y1': y1, 'X2': x2, 'Y2': y2}, index=[0])], ignore_index=True)\n",
    "            except IndexError:\n",
    "                print(\"Ошибка: Индекс за пределами диапазона. Пропуск обнаружения.\")\n",
    "\n",
    "    fps = 1 / (new_frame_time - prev_frame_time)\n",
    "    prev_frame_time = new_frame_time\n",
    "\n",
    "    # Запись кадра в выходное видео\n",
    "    out.write(img)\n",
    "\n",
    "    if have_display:\n",
    "        cv2.putText(img, f\"FPS: {int(fps)}\", (40, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        cv2.imshow(\"Изображение\", img)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "df.to_excel(r'detections3.xlsx', index=False)\n",
    "\n",
    "# Освобождение ресурсов VideoCapture и VideoWriter\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "if have_display:\n",
    "    cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "datalore": {
   "base_environment": "default",
   "computation_mode": "JUPYTER",
   "package_manager": "pip",
   "packages": [],
   "report_row_ids": [],
   "version": 3
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
